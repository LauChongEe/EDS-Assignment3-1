{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Assignment 3\n",
    "\n",
    "# scrape from lelong\n",
    "# find the pattern for the first page\n",
    "url = 'https://www.lelong.com.my/catalog/all/list?TheKeyword=macbook+pro&D='\n",
    "# write a loop to scrape from page 1 to the last page\n",
    "product_name=[]\n",
    "price_name=[]\n",
    "for page in range(1,20):\n",
    "    url_page = url+str(page)\n",
    "    scrape = requests.get(url_page)\n",
    "    soup = BeautifulSoup(scrape.content, 'lxml')\n",
    "    link = soup.find_all('div',{'class':'item','class':'summary'})\n",
    "    link1 = soup.find_all('div',{'class':'col total'})\n",
    "    length = len(link)\n",
    "    for i in range(0,length):\n",
    "        name = link[i].a.get('title')\n",
    "        product_name.append(name)\n",
    "        \n",
    "        #span_linlink1[i].find_all('span',{'class':'price pull-right'})\n",
    "        price = link1[i].div.span.b.string\n",
    "        price_name.append(price)\n",
    "        \n",
    "# write to csv\n",
    "# convert the list to a pandas dataframe\n",
    "\n",
    "df = pd.DataFrame({'name':product_name,'price':price_name})\n",
    "df['price'] = df.price.apply(lambda x : x.lstrip('RM '))\n",
    "df.to_csv('output.csv', index=False)\n",
    "df\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "# Ariff\n",
    "\n",
    "# scrape from lelong\n",
    "\n",
    "# find the pattern for the first page\n",
    "url = 'https://www.lelong.com.my/catalog/all/list/?TheKeyword=laptop&CategoryID=764&D='\n",
    "\n",
    "# write a loop to scrape from page 1 to the last page\n",
    "\n",
    "#     Product Name/Product Title  - done\n",
    "#     Amount/Price - done\n",
    "#     Brand - done\n",
    "#     Comments/Reviews - otw\n",
    "#     Number of views - done\n",
    "\n",
    "def get_type(name):\n",
    "    list_type = ['battery', 'adapter', 'keyboard']\n",
    "    for types in list_type:\n",
    "        types = str.lower(types)\n",
    "        name = str.lower(name)\n",
    "        regex = re.search(\"(\"+types+\")\", name)\n",
    "        if regex:\n",
    "            return str(regex.group())\n",
    "    return 'laptop'\n",
    "\n",
    "def get_brand(name):\n",
    "    list_brand = ['ASUS','ACER','HP COMPAQ','DELL','Lenovo','HP','OEM','Asus','COMPAQ','Acer','LENOVO','TOSHIBA','Dell','Toshiba','MSI']\n",
    "    for brand in list_brand:\n",
    "        brand = str.lower(brand)\n",
    "        name = str.lower(name)\n",
    "        regex = re.search(\"(\"+brand+\")\", name)\n",
    "        if regex:\n",
    "            return str(regex.group())\n",
    "    return 'no brand'\n",
    "\n",
    "product_name = []\n",
    "product_price = []\n",
    "product_view = []\n",
    "product_brand = []\n",
    "product_type = []\n",
    "for page in range(1,20):\n",
    "    url_page = url+str(page)\n",
    "    scrape = requests.get(url_page)\n",
    "    soup = BeautifulSoup(scrape.content, 'lxml')\n",
    "    link_a = soup.find_all('div',{'class':'item','class':'summary'})\n",
    "    link_view = soup.find_all('span', {'class':'hit fontsize12'})\n",
    "    length = len(link_a)\n",
    "    for i in range(0,length):\n",
    "        name = link_a[i].a.get('title')\n",
    "        price = link_a[i].a.b.get('data-price')\n",
    "        view = re.search(r'(\\d+)', link_view[i].get_text())\n",
    "        view = view.group(0)\n",
    "        brand = get_brand(name)\n",
    "        types = get_type(name)\n",
    "        product_name.append(name)\n",
    "        product_price.append('RM '+price)\n",
    "        product_view.append(view)\n",
    "        product_brand.append(brand)\n",
    "        product_type.append(types)\n",
    "# write to csv\n",
    "# convert the list to a pandas dataframe\n",
    "\n",
    "df = pd.DataFrame({'name':product_name, 'price':product_price, 'view':product_view, 'brand':product_brand, 'type':product_type})\n",
    "df\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "# YP\n",
    "\n",
    "# scrape from iproperty - one damansara\n",
    "\n",
    "# find the pattern for the first page\n",
    "url = 'https://www.iproperty.com.my/sale/all-residential/?q=one%20damansara&page='\n",
    "\n",
    "# write a loop to scrape from page 1 to the last page\n",
    "\n",
    "property_agent=[]\n",
    "property_price=[]\n",
    "property_built_up = []\n",
    "property_post_date = []\n",
    "\n",
    "for page in range(1,5):\n",
    "    url_page = url+str(page)\n",
    "    scrape = requests.get(url_page)\n",
    "    soup = BeautifulSoup(scrape.content, 'lxml')\n",
    "    link_name = soup.find_all('div',{'class':'dLCzlh'})\n",
    "    link_price = soup.find_all('div',{'class':'hzTrLN'})\n",
    "    link_built_up = soup.find_all('div',{'class':'crBzhV'})\n",
    "    link_post_date = soup.find_all('div',{'class':'ebSsFk'})\n",
    "\n",
    "    length=len(link_name)\n",
    "\n",
    "    for i in range(0,length):\n",
    "\n",
    "     \n",
    "        name = re.search('\">([\\w\\s]+)</div>', str(link_name[i]))\n",
    "        property_agent.append(name.group()[2:-6])\n",
    "        \n",
    "        price= re.search('RM\\s([\\w\\s,]+)<!--', str(link_price[i]))\n",
    "        property_price.append(price.group()[0:-4])\n",
    "        \n",
    "        built_up= re.search('\\s([0-9,]+)\\s([\\w\\s.]+)</a>', str(link_built_up[i]))\n",
    "        \n",
    "        if built_up:\n",
    "            property_built_up.append(built_up.group()[1:-4])\n",
    "\n",
    "        else:\n",
    "            property_built_up.append('Unknown')\n",
    "        \n",
    "        \n",
    "        post_date1 =re.search('today\\s([\\w\\s:]+)</p>', str(link_post_date[i]))\n",
    "        post_date2 =re.search('yesterday\\s([\\w\\s:]+)</p>', str(link_post_date[i]))\n",
    "        post_date3 = re.search('on\\s([\\w\\s:]+)</p>', str(link_post_date[i]))\n",
    "        \n",
    "\n",
    "        if post_date1:\n",
    "            property_post_date.append(post_date1.group()[0:-4])\n",
    "        elif post_date2:\n",
    "            property_post_date.append(post_date2.group()[0:-4])\n",
    "        elif post_date3:\n",
    "            property_post_date.append(post_date3.group()[3:-4])\n",
    "\n",
    "\n",
    "# write to csv\n",
    "# convert the list to a pandas dataframe\n",
    "\n",
    "df = pd.DataFrame({'Agent Name':property_agent, 'Price':property_price,'Built-up Area':property_built_up,'Posted Date': property_post_date})\n",
    "df\n",
    "df.to_csv('iproperty.csv', index=False)\n",
    "\n",
    "\n",
    "##################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
